<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="A novel method for real-world 3D detection and layout estimation from posed and unposed images">
    
    <link rel="icon" href="assets/favicon.ico">
    <title>TUN3D</title>
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="TUN3D â€” Realâ€‘World 3D Detection and Layout Estimation" />
    <meta property="og:description" content="A novel method for real-world 3D detection and layout estimation from posed and unposed images" />
    <meta property="og:image" id="ogImage" content="assets/img/scheme.png" />
    <meta property="og:video" id="ogVideo" content="" />
    <meta property="og:video:type" content="video/mp4" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="TUN3D â€” Realâ€‘World 3D Detection and Layout Estimation" />
    <meta name="twitter:description" content="A novel method for real-world 3D detection and layout estimation from posed and unposed images" />
    <meta name="twitter:image" id="twImage" content="assets/img/scheme.png" />
    <style>
      html, body { margin: 0; height: 100%; background: #fff; color: #333; font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, 'Noto Sans', sans-serif; }
      .wrap { width: 100%; margin: 0 auto; padding: 16px; display: grid; gap: 12px; }
      .paper { width: 80vw; margin: 0 auto; display: grid; gap: 12px; align-items: start; }
      .paper .abstract { background: #fff; border: 1px solid #ddd; border-radius: 10px; padding: 12px 16px; line-height: 1.5; font-size: 20px; }
      .paper .diagram { display: grid; place-items: center; }
      .paper .diagram img { width: 80%; margin-bottom: 8px; }
      .paper .diagram .caption { width: 80%;  font-size: 22px; color: #444; line-height: 1.5; text-align: center; }
      .video { width: 75vw; margin: 0 auto; display: grid; place-items: center; }
      .video video { width: 100%; height: auto; max-height: 45vh; border-radius: 8px; }
      .grid { width: 80vw; margin: 0 auto; display: grid; grid-template-columns: repeat(var(--cols, 3), 1fr); gap: 12px; justify-items: stretch; align-items: stretch; }
      .viewer { width: 100%; height: auto; aspect-ratio: 4 / 3; }
      .legend { font-size: 13px; opacity: 0.85; }
      a { color: #7cc4ff; }
      .title { text-align: center; }
      .scene_id { text-align: center; }
      .scene_id_container {margin: auto; display: grid; width: auto; grid-auto-flow: column; gap: 8px; align-items: center; justify-content: center; }
      .scene_button { padding: 6px 10px; border-radius: 6px; border: 1px solid #bbb; background: #fff; color: #333; cursor: pointer; max-width: 20vw; white-space: nowrap; 
        overflow: hidden; text-overflow: ellipsis; font-size: 22px; }
      .scene_button[aria-pressed="true"] { background: #2563eb; color: #fff; border-color: #2563eb; }
      .nav { width: 80vw; margin: 8px auto 16px; display: grid; grid-auto-flow: column; gap: 16px; justify-content: center; align-items: center; }
      .nav a { text-decoration: none; color: #2563eb; font-weight: 600; }
      .nav a:hover { text-decoration: underline; }
      .section-title { margin: 18px auto; width: 80vw; font-size: 27px; font-weight: 700; text-align: center; }
      .cta { width: 80vw; margin: 6px auto 10px; display: grid; grid-auto-flow: column; gap: 10px; justify-content: center; }
      .btn { display: inline-flex; align-items: center; gap: 8px; padding: 8px 12px; border-radius: 8px; font-weight: 600; text-decoration: none; border: 1px solid transparent; }
      .btn svg { height: 18px; width: auto; display: block; }
      .btn svg path { fill: currentColor !important; }
      .btn-gh { background: #fff; color: #111; border-color: #111; }
      .btn-gh svg path { fill: #000 !important; }
      .btn-gh:hover { background: #000; border-color: #000; color: #fff; }
      .btn-gh:hover svg path { fill: #fff !important; }
      .btn-arxiv { background: #b31b1b; color: #fff; border-color: #b31b1b; }
      .btn-arxiv:hover { background: #961515; border-color: #961515; }
      .btn-hf { background: #ed9d1b; color: #fff; border-color: #ff9d00; }
      .btn-hf:hover { background: #f48e00; border-color: #e88900; }
    @media (orientation: portrait) {
      .paper, .grid, .video { width: 92vw; }
      .section-title, .nav, .cta { width: 92vw; }
      .grid { grid-template-columns: 1fr; }
      .video video { max-height: 32vh; }
      .paper .diagram img { width: 100%; }
      .paper .diagram .caption { width: 100%; font-size: 18px; }
      .cta { grid-auto-flow: row; justify-content: center; justify-items: center; }
    }
    </style>
    <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.164.1/build/three.module.js"
        }
      }
    </script>
  </head>
  <body>
    <h1 class="title">TUN3D: Towards Real-World Scene Understanding from Unposed Images</h1>
    <div class="cta">
      <a class="btn btn-gh" href="https://github.com/" target="_blank" rel="noopener noreferrer">
        <svg width="18" height="18" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 98 96"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></svg>
        Code
      </a>
      <a class="btn btn-arxiv" href="https://arxiv.org/" target="_blank" rel="noopener noreferrer">
        <svg id="logomark" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17.732 24.269"><g id="tiny"><path d="M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z" transform="translate(-566.984 -271.548)" fill="#bdb9b4"/><path d="M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z" transform="translate(-566.984 -271.548)" fill="#b31b1b"/><path d="M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z" transform="translate(-566.984 -271.548)" fill="#bdb9b4"/></g></svg>
        arXiv
      </a>
      <a class="btn btn-hf" href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">
        <span style="font-size: 16px;">ðŸ¤—</span>
        Model Weights
      </a>
    </div>
    <nav class="nav">
      <a href="#demo">Demo</a>
      <a href="#abstract">Abstract</a>
      <a href="#method">Method</a>
      <a href="#examples">Examples</a>
    </nav>
    <h2 class="section-title" id="demo">Demo</h2>
    <div class="scene_id_container" id="scene_id_container"></div>
    <div class="wrap">
        <h3 class="scene_id" id="scene_id"></h3>
        <div class="grid" id="grid1"></div>
        <div class="video" id="videoWrap"></div>
    </div>
    <h2 class="section-title" id="abstract">Abstract</h2>
    <div class="paper">
      <div class="abstract">
        Layout estimation and 3D object detection are
        two fundamental tasks in indoor scene understanding. When
        combined, they enable the creation of a compact yet semanti-
        cally rich spatial representation of a scene. Existing approaches
        typically rely on point cloud input, which poses a major
        limitation since most consumer cameras lack depth sensors
        and visual-only data remains far more common. We address
        this issue with TUN3D, the first method that tackles joint
        layout estimation and 3D object detection in real scans, given
        multi-view images as input, and does not require ground-
        truth camera poses or depth supervision. Our approach builds
        on a lightweight sparse-convolutional backbone and employs
        two dedicated heads: one for 3D object detection and one for
        layout estimation, leveraging a novel and effective parametric
        wall representation. Extensive experiments show that TUN3D
        achieves state-of-the-art performance across three challenging
        scene understanding benchmarks: (i) using ground-truth point
        clouds, (ii) using posed images, and (iii) using unposed images.
        While performing on par with specialized 3D object detection
        methods, TUN3D significantly advances layout estimation, set-
        ting a new benchmark in holistic indoor scene understanding.
      </div>
      <h2 class="section-title" id="method">Method</h2>
      <div class="diagram">
        <img src="assets/img/scheme.png" alt="Method diagram" />
        <div class="caption">
          (A) TUN3D can flexibly process various inputs: unposed images, posed images, and point clouds. (B) TUN3D model is constructed of a 3D sparse-convolutional backbone and neck, followed by two task-specific heads. (C) The novel <span style="color: #00afff; font-weight: bold;">layout</span> head predicts wall scores and regresses wall parameters for each wall comprising the <span style="color: #00afff; font-weight: bold;">layout</span>. (D) The detection head outputs <span style="color: #e6d017; font-weight: bold;">object</span> class scores and coordinates of a 3D bounding box of an <span style="color: #e6d017; font-weight: bold;">object</span>.
        </div>
      </div>
    </div>
    <h2 class="section-title" id="examples">Examples</h2>
    <div class="paper">
      <div class="diagram">
        <img src="assets/img/scannet_res.png" alt="ScanNet Results" />
        <div class="caption">
          Ground truth and predicted <span style="color: #00afff; font-weight: bold;">layouts</span> and <span style="color: #e6d017; font-weight: bold;">objects</span> on ScanNet dataset.
        </div>
      </div>
      <div class="diagram">
        <img src="assets/img/s3dis_res.png" alt="S3DIS Results" />
        <div class="caption">
          Ground truth and predicted <span style="color: #00afff; font-weight: bold;">layouts</span> and <span style="color: #e6d017; font-weight: bold;">objects</span> on S3DIS dataset.
        </div>
      </div>
    </div>

    <script type="module">
      import { createPLYViewer, loadAnnotationsFromJson, loadPosesFromTxt } from './src/PLYModelViewer.js';

      const possibleSceneIds = ['scene0356_02', 'scene0338_00', 'scene0146_02'];
      const sceneIdContainer = document.getElementById('scene_id_container');
      const grid = document.getElementById('grid1');
      const videoWrap = document.getElementById('videoWrap');

      let sceneId = possibleSceneIds[0];
      let currentVideo = null;
      let currentPoses = null;

      function setActiveButton(value) {
        for (const btn of sceneIdContainer.querySelectorAll('button.scene_button')) {
          const pressed = btn.value === value;
          btn.setAttribute('aria-pressed', pressed ? 'true' : 'false');
        }
      }

      function loadVideo(container, sceneId) {
        if (!container) return;
        const video = document.createElement('video');
        video.src = `data/${sceneId}/video.mp4`;
        video.controls = true;
        video.playsInline = true;
        video.muted = true; // allow autoplay
        video.autoplay = true;
        video.loop = true;
        video.preload = 'auto';
        container.innerHTML = '';
        container.appendChild(video);
        currentVideo = video;
        const tryPlay = () => {
          const p = video.play();
          if (p && typeof p.then === 'function') {
            p.catch(() => { video.muted = true; video.play().catch(() => {}); });
          }
        };
        if (document.visibilityState === 'visible') tryPlay();
        else document.addEventListener('visibilitychange', () => { if (document.visibilityState === 'visible') tryPlay(); }, { once: true });
      }

      function disposeGrid(container) {
        if (!container) return;
        for (const child of Array.from(container.children)) {
          const wrapper = child.firstElementChild;
          const viewer = wrapper && wrapper.__viewer;
          if (viewer && typeof viewer.dispose === 'function') viewer.dispose();
        }
        container.innerHTML = '';
      }

      async function makeGrid(container, sceneId) {
        if (!container) return;
        let types = {
            'Point Cloud': {pcd: 'scene_gt', bboxes: 'bboxes_pred_gt_pcd', layout: 'bboxes_layout_pred_gt_pcd'},
            'Posed RGB': {pcd: 'scene_posed', bboxes: 'bboxes_posed_pred', layout: 'bboxes_layout_posed_pred'},
            'Unposed RGB': {pcd: 'scene_unposed', bboxes: 'bboxes_unposed_pred', layout: 'bboxes_layout_unposed_pred'},
            'Ground Truth': {pcd: 'scene_gt', bboxes: 'bboxes_gt', layout: 'bboxes_layout_gt'}
        }
        const groupId = `demo-group-${sceneId}`;
        const options = { width: '100%', height: '100%', showAnnotations: true };
        const names = Object.keys(types);
        container.style.setProperty('--cols', String(names.length));

        // Load poses for this scene once
        try {
          currentPoses = await loadPosesFromTxt(`data/${sceneId}/poses.txt`);
        } catch (e) {
          currentPoses = null;
          console.warn('Poses not loaded:', e.message);
        }

        for (const name of names) {
            const viewerDiv = document.createElement('div');
            viewerDiv.className = 'viewer';
            container.appendChild(viewerDiv); // keep order stable
            (async () => {
                const model = `data/${sceneId}/${types[name].pcd}.ply`;
                const bboxes = await loadAnnotationsFromJson(`data/${sceneId}/${sceneId}_${types[name].bboxes}.json`);
                let layout = await loadAnnotationsFromJson(`data/${sceneId}/${sceneId}_${types[name].layout}.json`);
                layout = layout.map(item => [item[0], item[1], item[2], item[3], item[4], item[5], 18]);
                const annotations = [...bboxes, ...layout];
                const wrapper = await createPLYViewer(model, annotations, { ...options }, groupId, name);
                viewerDiv.appendChild(wrapper);
                if (wrapper.__viewer) {
                  wrapper.__viewer.setSyncWithVideoEnabled(true, true);
                }
                // attach video sync if poses/video available
                if (currentVideo && currentPoses && wrapper.__viewer) {
                  wrapper.__viewer.attachVideoSync(currentVideo, currentPoses);
                  wrapper.__viewer.setSyncWithVideoEnabled(true, true);
                }
            })();
        }
      }

      function selectScene(newSceneId) {
        if (sceneId === newSceneId) return;
        sceneId = newSceneId;
        document.getElementById('scene_id').textContent = sceneId;
        setActiveButton(sceneId);
        disposeGrid(grid);
        loadVideo(videoWrap, sceneId);
      // Update social preview: prefer video if available, else fallback to first frame
      try {
        const ogVideo = document.getElementById('ogVideo');
        const ogImage = document.getElementById('ogImage');
        const twImage = document.getElementById('twImage');
        const videoUrl = `data/${sceneId}/video.mp4`;
        if (ogVideo) ogVideo.setAttribute('content', videoUrl);
        if (ogImage) ogImage.setAttribute('content', videoUrl);
        if (twImage) twImage.setAttribute('content', videoUrl);
      } catch (_) {}

        makeGrid(grid, sceneId);
      }

      // initial render
      for (let id of possibleSceneIds) {
        const button = document.createElement('button');
        button.className = 'scene_button';
        button.value = id;
        button.textContent = id;
        button.setAttribute('aria-pressed', id === sceneId ? 'true' : 'false');
        button.addEventListener('click', () => selectScene(id));
        sceneIdContainer.appendChild(button);
      }
      setActiveButton(sceneId);
      document.getElementById('scene_id').textContent = sceneId;
      loadVideo(videoWrap, sceneId);
      // Initialize social preview for initial scene
      try {
        const ogVideo = document.getElementById('ogVideo');
        const ogImage = document.getElementById('ogImage');
        const twImage = document.getElementById('twImage');
        const videoUrl = `data/${sceneId}/video.mp4`;
        if (ogVideo) ogVideo.setAttribute('content', videoUrl);
        if (ogImage) ogImage.setAttribute('content', videoUrl);
        if (twImage) twImage.setAttribute('content', videoUrl);
      } catch (_) {}
      makeGrid(grid, sceneId);
    </script>
  </body>
</html> 